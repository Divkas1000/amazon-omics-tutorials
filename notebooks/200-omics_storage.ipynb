{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Omics Storage with genomic references and readsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal of this notebook is to get you acquainted with Omics Storage. If you complete this notebook you will have:\n",
    "1. Created a Reference Store\n",
    "2. Imported a Reference Genome\n",
    "3. Created a Sequence Store\n",
    "4. Imported several FASTQ files\n",
    "5. Imported a CRAM file\n",
    "6. Downloaded a ReadSet using the Omics Transfer Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Python requirements\n",
    "* Python >= 3.8\n",
    "* Packages:\n",
    "  * boto3 >= 1.26.19\n",
    "  * botocore >= 1.29.19\n",
    "\n",
    "### AWS requirements\n",
    "\n",
    "#### AWS CLI\n",
    "You will need the AWS CLI installed and configured in your environment. Supported AWS CLI versions are:\n",
    "\n",
    "* AWS CLI v2 >= 2.9.3 (Recommended)\n",
    "* AWS CLI v1 >= 1.27.19\n",
    "\n",
    "#### AWS Region\n",
    "Amazon Omics is currently available in Oregon (us-west-2), N. Virginia (us-east-1), Dublin (eu-west-1), London (eu-west-2), Frankfurt (eu-central-1), and Singapore (ap-southeast-1).\n",
    "\n",
    "**================**\n",
    "\n",
    "**As written, this notebook works best in the `us-east-1` AWS Region.** The datasets it uses are all openly available via the [Registry of Open Data on AWS](https://registry.opendata.aws) and are stored in buckets in `us-east-1`.\n",
    "\n",
    "**================**\n",
    "\n",
    "**Amazon Omics only allows importing data within the same region.** For this notebook to work in other regions, you will need to have the following data available in an S3 bucket in your selected region:\n",
    "\n",
    "* s3://1000genomes-dragen-v3.7.6/references/fasta/hg38.fa\n",
    "* s3://1000genomes/1000G_2504_high_coverage/additional_698_related/data/ERR3988761/HG00405.final.cram*\n",
    "* s3://giab/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/140407_D00360_0016_AH948VADXX/Project_RM8398/Sample_U0a/U0a_CGATGT_L001_R{1,2}_*.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initial Set Up\n",
    "First, we need to set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools as it\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "from pprint import pprint\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a service IAM role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will use the following policy and trust policy to demo the capabilities of Amazon Omics, you are free to customize permissions as required for your use case after going though this tutorial.\n",
    "\n",
    "**NOTE:**\n",
    "In this case we've defined rather permissive permissions (i.e. \"\\*\" Resources). In particular, we are allowing read/write access to all S3 buckets available to the account for this tutorial. In a real world setting you will want to scope this down to only the minimally needed actions on necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a timestamp\n",
    "dt_fmt = '%Y%m%dT%H%M%S'\n",
    "ts = datetime.now().strftime(dt_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"omics:*\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ram:AcceptResourceShareInvitation\",\n",
    "        \"ram:GetResourceShareInvitations\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:GetBucketLocation\",\n",
    "        \"s3:PutObject\",\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:ListBucket\",\n",
    "        \"s3:AbortMultipartUpload\",\n",
    "        \"s3:ListMultipartUploadParts\",\n",
    "        \"s3:GetObjectAcl\",\n",
    "        \"s3:PutObjectAcl\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "demo_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"omics.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to proceed we need to create a couple of resources. The first is the role that you will be passing into Amazon Omics. If the role doesn't exist, we will need to create it and attach the policy and trust policy defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this as the base name for our role and policy\n",
    "omics_iam_name = f'OmicsTutorialRole-{ts}'\n",
    "\n",
    "# Create the iam client\n",
    "iam = boto3.resource('iam')\n",
    "\n",
    "# Check if the role already exist if not create it\n",
    "try:\n",
    "    role = iam.Role(omics_iam_name)\n",
    "    role.load()\n",
    "    \n",
    "except botocore.exceptions.ClientError as ex:\n",
    "    if ex.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n",
    "        #Create the role with the corresponding trust policy\n",
    "        role = iam.create_role(\n",
    "            RoleName=omics_iam_name, \n",
    "            AssumeRolePolicyDocument=json.dumps(demo_trust_policy))\n",
    "        \n",
    "        #Create policy\n",
    "        policy = iam.create_policy(\n",
    "            PolicyName='{}-policy'.format(omics_iam_name), \n",
    "            Description=\"Policy for Amazon Omics demo\",\n",
    "            PolicyDocument=json.dumps(demo_policy))\n",
    "        \n",
    "        #Attach the policy to the role\n",
    "        policy.attach_role(RoleName=omics_iam_name)\n",
    "    else:\n",
    "        print('Somthing went wrong, please retry and check your account settings and permissions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the role exists, lets create a helper function to help us retrieve the role arn which we will need to pass into the service API calls. The role arn will grant Amazon Omics the proper permissions to access the resources it needs in your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_arn(role_name):\n",
    "    try:\n",
    "        iam = boto3.resource('iam')\n",
    "        role = iam.Role(role_name)\n",
    "        role.load()  # calls GetRole to load attributes\n",
    "    except botocore.exeptions.ClientError:\n",
    "        print(\"Couldn't get role named %s.\"%role_name)\n",
    "        raise\n",
    "    else:\n",
    "        return role.arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a dictionary of the datasources we'll be using in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_S3_URI_TPL = {\n",
    "    \"reads\": {\n",
    "        \"fastq\": \"s3://giab/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/140407_D00360_0016_AH948VADXX/Project_RM8398/Sample_U0a/U0a_CGATGT_L001_R{read}_{part:03}.fastq.gz\",\n",
    "    }\n",
    "}\n",
    "SOURCE_S3_URIS = {\n",
    "    \"reference\": \"s3://1000genomes-dragen-v3.7.6/references/fasta/hg38.fa\",\n",
    "    \"reads\": {\n",
    "        \"fastq\": [\n",
    "            SOURCE_S3_URI_TPL['reads']['fastq'].format(read=read, part=part) \n",
    "            for read, part, in list(it.product([1,2], [1,2,3,4]))],\n",
    "        \"cram\": [\n",
    "            \"s3://1000genomes/1000G_2504_high_coverage/additional_698_related/data/ERR3988761/HG00405.final.cram\",\n",
    "            \"s3://1000genomes/1000G_2504_high_coverage/additional_698_related/data/ERR3988762/HG00408.final.cram\",\n",
    "            \"s3://1000genomes/1000G_2504_high_coverage/additional_698_related/data/ERR3988763/HG00418.final.cram\",\n",
    "            \"s3://1000genomes/1000G_2504_high_coverage/additional_698_related/data/ERR3988764/HG00420.final.cram\",\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Amazon Omics client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = boto3.client('omics', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first going to import a genome reference. This reference will be taken from 1000 genomes, which will also be the source of the reads. This import has two steps. First, if this is the first time you've created a reference, you first must create a reference store.\n",
    "\n",
    "Reference Stores (and Seqeuence Stores) also support CMKs; however, we'll use AWS-owned encryption keys throughout.\n",
    "\n",
    "To start lets create a helper method to retrieve the reference store id and have it return empty if it doesn't exist. There should only be one reference store per region per account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_store_id(client=None):\n",
    "    if not client:\n",
    "        client = boto3.client('omics')\n",
    "    \n",
    "    resp = client.list_reference_stores(maxResults=10)\n",
    "    list_of_stores = resp.get('referenceStores')\n",
    "    store_id = None\n",
    "    \n",
    "    if list_of_stores != None:\n",
    "        # As mentioned above there can only be one store per region\n",
    "        # if there is a store present is the first one\n",
    "        store_id = list_of_stores[0].get('id')\n",
    "    \n",
    "    return store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Checking for a reference store in region: {omics.meta.region_name}\")\n",
    "if get_ref_store_id(omics) == None:\n",
    "    response = omics.create_reference_store(name='myReferenceStore')\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"Congratulations, you have an existing reference store!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now import a reference using the `start_reference_import_job` API call.\n",
    "\n",
    "This will use the reference store we created (or found) and the IAM role we created above. All references in a Reference store must have a unique name. So, we're also going to apply a timestamp to the reference name to ensure that it is unique.\n",
    "\n",
    "Also note that the IAM role used should have the ability to read the object in the S3 bucket that your reference is sourced from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_name = f'tutorial-1kg-grch38-{ts}'\n",
    "\n",
    "ref_import_job = omics.start_reference_import_job(\n",
    "    referenceStoreId=get_ref_store_id(omics), \n",
    "    roleArn=get_role_arn(omics_iam_name),\n",
    "    sources=[{\n",
    "        'sourceFile': SOURCE_S3_URIS[\"reference\"],\n",
    "        'name': ref_name,\n",
    "        'tags': {'SourceLocation': '1kg'}\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status here either using `omics.get_reference_import_job` or by navigating to the [Console](https://us-east-1.console.aws.amazon.com/omics/home?region=us-east-1#/reference/referenceStoreDetail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_import_job = omics.get_reference_import_job(\n",
    "    referenceStoreId=get_ref_store_id(omics), \n",
    "    id=ref_import_job['id'])\n",
    "ref_import_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import can take up to 5 minutes to complete. We can wait for it to complete using a `waiter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"waiting for job {ref_import_job['id']} to complete\")\n",
    "try:\n",
    "    waiter = omics.get_waiter('reference_import_job_completed')\n",
    "    waiter.wait(referenceStoreId=ref_import_job['referenceStoreId'], id=ref_import_job['id'])\n",
    "\n",
    "    print(f\"job {ref_import_job['id']} complete\")\n",
    "except botocore.exceptions.WaiterError as e:\n",
    "    print(f\"job {ref_import_job['id']} FAILED:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the import job is complete, we should be able to see our reference in the list of available references in the Reference store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = omics.list_references(referenceStoreId=get_ref_store_id(omics), filter={\"name\": ref_name})\n",
    "\n",
    "ref_list = resp\n",
    "pprint(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can get more details for our imported reference with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing this specific reference as we'll be using it later\n",
    "ref = omics.get_reference_metadata(\n",
    "    referenceStoreId=get_ref_store_id(omics), \n",
    "    id=ref_list['references'][0]['id'])\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've imported your reference, let's import our first set of genomics reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create our first sequence store. A sequence store is similar to an S3 bucket; it holds a set of objects, known as read sets. \n",
    "\n",
    "The easiest way to think about a read set is it an abstraction of a set of genomics file types (FASTQ, BAM, CRAM) that store reads from a sequencer. When you get a read set back from a sequence store and process it, the data you get back is semantically identical to what you put in.\n",
    "\n",
    "Note that you can use customer managed CMKs with Omics storage; these are managed at the store level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sequence stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sequence store and retrieve its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_store_name = f'tutorial-seq-store-{ts}'\n",
    "response = omics.create_sequence_store(name=sequence_store_name)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqstore = response\n",
    "omics.get_sequence_store(id=seqstore['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import sequence data\n",
    "\n",
    "Now, let's import some data in the form of FASTQs and CRAMs. Amazon Omics allows you to provide additional metadata for your reads using import manifests. Let's look at how to generate manifests for CRAM and FASTQ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRAM manifests\n",
    "CRAM manifests are relatively simple since all the read data is typically in one source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = get_role_arn(omics_iam_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of cram sources\n",
    "cram_sources = []\n",
    "for source_uri in SOURCE_S3_URIS['reads']['cram']:\n",
    "    source = urllib.parse.urlparse(source_uri)\n",
    "    subject_id = source.path[1:].split('/')[-2]  # accession, e.g. ERR1234567\n",
    "    sample_id = source.path[1:].split('/')[-1].split('.')[0] # basename of object, e.g. HG12345\n",
    "    \n",
    "    cram_sources += [{\n",
    "        'subjectId': subject_id,\n",
    "        'sampleId': sample_id,\n",
    "        'sourceFileType': 'CRAM',\n",
    "        'sourceFiles': { 'source1': source_uri },\n",
    "        'referenceArn': ref['arn'],  # we stashed this earlier\n",
    "        'generatedFrom': source_uri,\n",
    "        'description': f'{subject_id}/{sample_id} from {source.netloc}',\n",
    "        'tags': {'SourceLocation': source.netloc}\n",
    "    }]\n",
    "cram_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FASTQ manifests\n",
    "FASTQs can be a bit more complicated since read data can come from one or two source files. Additionally, sources for multiple paired-end reads may exist in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the set of source URIs is evenly matched with two reads {R1, R2} and for parts each\n",
    "# so we can just sort and divide the list into two to get matched lists of paired reads\n",
    "source_uris = zip(\n",
    "    sorted(SOURCE_S3_URIS['reads']['fastq'])[:len(SOURCE_S3_URIS['reads']['fastq'])//2],\n",
    "    sorted(SOURCE_S3_URIS['reads']['fastq'])[len(SOURCE_S3_URIS['reads']['fastq'])//2:]\n",
    ")\n",
    "\n",
    "fastq_sources = []\n",
    "for reads in source_uris:\n",
    "    source = urllib.parse.urlparse(reads[0])\n",
    "    subject_id = source.path[1:].split('/')[1]  # NA12878\n",
    "    sample_id = source.path[1:].split('/')[5]  # Sample_U0a\n",
    "    \n",
    "    fastq_sources += [{\n",
    "        'subjectId': subject_id,\n",
    "        'sampleId': sample_id,\n",
    "        'sourceFileType': 'FASTQ',\n",
    "        'sourceFiles': { \n",
    "            'source1': reads[0],\n",
    "            'source2': reads[1]\n",
    "        },\n",
    "        'referenceArn': ref['arn'],  # we stashed this earlier\n",
    "        'generatedFrom': f'{subject_id}/{sample_id} fastq',\n",
    "        'description': f'{subject_id}/{sample_id} from {source.netloc}',\n",
    "        'tags': {'SourceLocation': source.netloc}\n",
    "    }]\n",
    "fastq_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing ReadSets\n",
    "\n",
    "Now, we'll make a combined set of import sources for our store. You can mix and match read sets of different types with imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_sources = fastq_sources + cram_sources\n",
    "import_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readset_import_job = omics.start_read_set_import_job(\n",
    "    roleArn=get_role_arn(omics_iam_name),\n",
    "    sequenceStoreId=seqstore['id'], \n",
    "    sources=import_sources)\n",
    "readset_import_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor this progress with `omics.get_read_set_import_job`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readset_import_job = omics.get_read_set_import_job(\n",
    "    sequenceStoreId=seqstore['id'], \n",
    "    id=readset_import_job['id'])\n",
    "\n",
    "readset_import_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the size of the source data, ReadSet import jobs can take up to 30min for each source to complete.\n",
    "\n",
    "The import of an individual Read Set into a sequence store will take longer than a traditional S3 copy. This is because Amazon Omics will validate files, calculate metadata, and organize your ReadSets to make it easier for you to consume and share.\n",
    "\n",
    "Amazon Omics supports high parallelism across imports. For example, if the import of a FASTQ takes X minutes, the import of many FASTQs of the same size should also take approximately X minutes. Each import job supports a maximum of 1000 read sets. So we could import all of 1000 genomes with only 4 API calls. For higher parallelism, you can split larger datastes into multiple ReadSet import jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying References and ReadSets to a local file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many existing tools for genomics data processing expect to read files on the local filesystem.\n",
    "\n",
    "Amazon Omics supports multipart downloads much like S3, but specifically designed for genomic datatypes. After importing your references and readsets into Amazon Omics Storage, you can easily retrieve them and their supporting files (e.g. index files like `*.fai` for references, or `*.crai` for reads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list = omics.list_references(referenceStoreId=get_ref_store_id(omics))\n",
    "readset_list = omics.list_read_sets(sequenceStoreId=seqstore['id'])\n",
    "\n",
    "pprint(ref_list)\n",
    "pprint(readset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Omics provides a number of basic methods to retrieve data. For efficiency, references and read sets are stored in multiple parts. We can see this if we look at one of the references we imported earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_metadata = omics.get_reference_metadata(referenceStoreId=ref['referenceStoreId'], id=ref['id'])\n",
    "ref_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `files` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_metadata['files']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the `source` file has several parts. There is also a smaller `index` file which will typically have only one part.\n",
    "\n",
    "The Amazon Omics API `get_reference` lets you download one part at a time. Let's use it to download pieces of the reference and its associated index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a part of the reference\n",
    "response = omics.get_reference(referenceStoreId=ref['referenceStoreId'], id=ref['id'], partNumber=1, file='SOURCE')\n",
    "with open('reference.part.fa', 'wb') as f:\n",
    "    f.write(response['payload'].read())\n",
    "\n",
    "# download a part of the reference index\n",
    "response = omics.get_reference(referenceStoreId=ref['referenceStoreId'], id=ref['id'], partNumber=1, file='INDEX')\n",
    "with open('reference.part.fai', 'wb') as f:\n",
    "    f.write(response['payload'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are similar APIs for downloading ReadSets. One notable difference is that ReadSets can have more than one source - e.g. `SOURCE1` and `SOURCE2` - and no index if they originate from FASTQ, or only a single source and an accompanying index if they come from an aligned format like BAM or CRAM.\n",
    "\n",
    "> **Note** the following cells will only work if you have an available readset. The first cell below will check for one. Subsequent cells thereafter assume a readset was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are readsets available\n",
    "readset_import_jobs = omics.list_read_set_import_jobs(sequenceStoreId=seqstore['id'], filter={'status': 'IN_PROGRESS'})\n",
    "readset_list = omics.list_read_sets(sequenceStoreId=seqstore['id'])\n",
    "if not readset_list.get('readSets'):\n",
    "    print(f\"no active readsets available in {seqstore['id']}.\")\n",
    "    if readset_import_jobs.get('importJobs'):\n",
    "        print(f\"{len(readset_import_jobs.get('importJobs'))} in progress readset import jobs found. Readsets should be available in a few minutes.\")\n",
    "else:\n",
    "    readset = readset_list['readSets'][0]\n",
    "    pprint(readset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readset_metadata = omics.get_read_set_metadata(sequenceStoreId=readset['sequenceStoreId'], id=readset['id'])\n",
    "readset_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = omics.get_read_set(sequenceStoreId=readset['sequenceStoreId'], id=readset['id'], partNumber=1, file='SOURCE1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a part of the readset\n",
    "response = omics.get_read_set(sequenceStoreId=readset['sequenceStoreId'], id=readset['id'], partNumber=1, file='SOURCE1')\n",
    "with open(f\"reads1.part.{readset_metadata['fileType'].lower()}\", 'wb') as f:\n",
    "    f.write(response['payload'].read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
